网站地址：https://textual-inversion.github.io/

github项目链接：https://github.com/rinongal/textual_inversion

项目简介（翻译）：

整个项目通过学习，生成特定的概念，来构造私人定制式样的物品或构成独特的艺术风格，这个过程，是通过在预训练的“文本到图像模型”的嵌入空间中得到的“新的 单词”来描述。
得到的“new words”是可以在新的句子中应用（这里我认为应该是指重新应用于“文本-图像”的模型，作为新的输入文字，来构造新的模型。）
整个项目是基于公开可用的——Latent Diffusion Models

项目摘要（翻译）：
Text-to-image models对于通过自然语言指导图像的创造生成，提供了前所未有的自由度。
但是，对于如何将这种自由度应用在生成具有特定概念（或者说风格也行，我个人觉得）的图片上，尚且还存在疑惑，比如如何调整图片的外表，或者把图片塞入新的角色和场景里去（
这里我觉得就是文章提供的model所解决的问题，比如我们输入一张莫奈风格的画作，然后我们希望，把这个作品放到马克杯上。这个放到马克杯的过程，就是塞入新的场景中去。）
换句话说，我们想知道，怎样我们才能利用自然语言指导的模型，来把我们的猫，变成一幅画；或者基于我们最喜爱的玩具，来构造一个产品。
这里，我们呈现了一种示例方法，这种方法能够实现以上的想法。

使用仅仅3-5张基于用户提供的概念生成的图片，可以是具体的物品或者说只是一种风格，我们学习如何在一个frozen text-to-image model的嵌入空间中，用“new words”来
表征这个风格或者物品。（全文的话，似乎也经常用concept来代指。）
这些“new words”可以被组装进自然语言句子，用来以一种直观的方式，指导个性化创造的生成。
尤其,我们发现一个简单的word embedding对于捕捉独特和多样的概念来说，是足够的。（我这里的理解是，一个embedding（S*）就足以捕捉输入图片的基本特性。） 
我们将我们的方法与一种baseline作比较，最终确认，我们的方法可以更准确（原文是faithfully，这里我觉得翻译成中文来看，准确更合适。）
地在多样的应用和任务中化用我们提供的概念。

【how does it work】模型原理这一部分，就直接放原文，感觉怎么翻译都不如原文流畅。
In the text-encoding stage of most text-to-image models, the first stage involves converting the prompt into a numerical representation. 
This is typically done by converting the words into tokens, each equivalent to an entry in the model's dictionary. 
These entries are then converted into an "embedding" - a continuous vector representation for the specific token. 
These embeddings are usually learned as part of the training process. 
In our work, we find new embeddings that represent specific, user-provided visual concepts. 重点就是在于，给用户提供地visual concept，再将其抽象成S*。
These embeddings are then linked to new pseudo-words, which can be incorporated into new sentences like any other word. 
In a sense, we are performing inversion into the text-embedding space of the frozen model. We're calling the process 'Textual Inversion'.
